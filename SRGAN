{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":588358,"sourceType":"datasetVersion","datasetId":286056}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Set CUDA_VISIBLE_DEVICES to restrict TensorFlow to only use the first GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n\n# Verify that TensorFlow sees only one GPU\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    print(f\"Using GPU: {physical_devices[0]}\")\nelse:\n    print(\"No GPU found. Using CPU.\")\n\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nprint(f\"TensorFlow version: {tf.__version__}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-03-19T00:08:02.905118Z","iopub.execute_input":"2025-03-19T00:08:02.905468Z","iopub.status.idle":"2025-03-19T00:08:14.017657Z","shell.execute_reply.started":"2025-03-19T00:08:02.905439Z","shell.execute_reply":"2025-03-19T00:08:14.016751Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\nTensorFlow version: 2.13.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class SRGANConfig:\n    def __init__(self):\n        # Dataset parameters\n        self.dataset_name = \"DIV2K\"\n        self.data_dir = \"../input/div2k-dataset/DIV2K_train_HR/DIV2K_train_HR\"\n        self.val_dir = \"../input/div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\"\n        self.hr_height = 256\n        self.hr_width = 256\n        self.scale_factor = 4\n        self.lr_height = self.hr_height // self.scale_factor\n        self.lr_width = self.hr_width // self.scale_factor\n        \n        # Training parameters\n        self.epochs = 100\n        self.batch_size = 16\n        self.gen_lr = 1e-4\n        self.disc_lr = 1e-4\n        self.beta1 = 0.5\n        \n        # Model parameters\n        self.gen_filters = 64\n        self.disc_filters = 64\n        self.num_res_blocks = 16\n        \n        # Loss weights\n        self.content_weight = 1.0\n        self.adversarial_weight = 1e-3\n        \n        # Paths\n        self.checkpoint_dir = \"checkpoints\"\n        self.sample_dir = \"samples\"\n        \n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        os.makedirs(self.sample_dir, exist_ok=True)\n\nconfig = SRGANConfig()\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T00:08:14.019125Z","iopub.execute_input":"2025-03-19T00:08:14.019648Z","iopub.status.idle":"2025-03-19T00:08:14.026034Z","shell.execute_reply.started":"2025-03-19T00:08:14.019622Z","shell.execute_reply":"2025-03-19T00:08:14.025171Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class DataLoader:\n    def __init__(self, config):\n        self.config = config\n        \n    def _verify_paths(self):\n        # Check if directories exist\n        if not tf.io.gfile.exists(self.config.data_dir):\n            raise ValueError(f\"Training directory {self.config.data_dir} does not exist\")\n        if not tf.io.gfile.exists(self.config.val_dir):\n            raise ValueError(f\"Validation directory {self.config.val_dir} does not exist\")\n            \n        # Check if files exist\n        train_files = tf.io.gfile.glob(os.path.join(self.config.data_dir, \"*.png\"))\n        val_files = tf.io.gfile.glob(os.path.join(self.config.val_dir, \"*.png\"))\n        \n        if not train_files:\n            raise ValueError(f\"No PNG files found in {self.config.data_dir}\")\n        if not val_files:\n            raise ValueError(f\"No PNG files found in {self.config.val_dir}\")\n            \n        return len(train_files), len(val_files)\n    \n    def load_and_preprocess(self, image_path):\n        img = tf.io.read_file(image_path)\n        img = tf.image.decode_png(img, channels=3)\n        img = tf.image.random_crop(img, [self.config.hr_height, self.config.hr_width, 3])\n        img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n        \n        # Create low-res version\n        lr_img = tf.image.resize(img, [self.config.lr_height, self.config.lr_width], \n                               method='bicubic')\n        return lr_img, img\n    \n    def create_dataset(self, is_training=True):\n        # Verify paths and get file counts\n        train_count, val_count = self._verify_paths()\n        \n        # Create dataset\n        if is_training:\n            files = tf.data.Dataset.list_files(os.path.join(self.config.data_dir, \"*.png\"), \n                                             shuffle=True)\n            print(f\"Found {train_count} training images\")\n        else:\n            files = tf.data.Dataset.list_files(os.path.join(self.config.val_dir, \"*.png\"), \n                                             shuffle=False)\n            print(f\"Found {val_count} validation images\")\n            \n        dataset = files.map(self.load_and_preprocess, \n                          num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.batch(self.config.batch_size)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset\n\n# Usage\ndata_loader = DataLoader(config)\ntrain_dataset = data_loader.create_dataset(is_training=True)\nval_dataset = data_loader.create_dataset(is_training=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T00:09:03.393752Z","iopub.execute_input":"2025-03-19T00:09:03.394571Z","iopub.status.idle":"2025-03-19T00:09:03.664881Z","shell.execute_reply.started":"2025-03-19T00:09:03.394540Z","shell.execute_reply":"2025-03-19T00:09:03.664023Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found 800 training images\nFound 100 validation images\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n    def residual_block(x, filters):\n        shortcut = x\n        x = layers.Conv2D(filters, 3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.PReLU(shared_axes=[1, 2])(x)\n        x = layers.Conv2D(filters, 3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Add()([shortcut, x])\n        return x\n    \n    def upsample_block(x, filters):\n        x = layers.Conv2D(filters * 4, 3, padding='same')(x)\n        x = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2))(x)\n        x = layers.PReLU(shared_axes=[1, 2])(x)\n        return x\n    \n    def build_generator(config):\n        inputs = layers.Input(shape=(config.lr_height, config.lr_width, 3))\n        x = layers.Conv2D(64, 9, padding='same')(inputs)\n        x = layers.PReLU(shared_axes=[1, 2])(x)\n    \n        shortcut = x\n    \n        for _ in range(16):\n            x = residual_block(x, 64)\n    \n        x = layers.Conv2D(64, 3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Add()([shortcut, x])\n    \n        for _ in range(2):\n            x = upsample_block(x, 64)\n    \n        outputs = layers.Conv2D(3, 9, padding='same', activation='tanh')(x)\n    \n        return keras.Model(inputs, outputs)\n    \n    generator = build_generator(config)\n    generator.summary()\n\n\n    def build_discriminator(config):\n        inputs = layers.Input(shape=(config.hr_height, config.hr_width, 3))\n        \n        # Initial convolutional layer\n        x = layers.Conv2D(64, kernel_size=3, strides=1, padding=\"same\")(inputs)\n        x = layers.LeakyReLU(alpha=0.2)(x)\n    \n        # Series of convolutional blocks with increasing filters\n        for i in range(4):\n            filters = min(64 * (2 ** i), 512)  # Cap filters at 512\n            strides = 1 if i % 2 else 2       # Alternate strides between 1 and 2\n            x = layers.Conv2D(filters=filters,\n                              kernel_size=3,\n                              strides=strides,\n                              padding=\"same\")(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.LeakyReLU(alpha=0.2)(x)\n    \n        # Fully connected layer for classification\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(1024)(x)\n        x = layers.LeakyReLU(alpha=0.2)(x)\n        \n        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n        return keras.Model(inputs, outputs)\n    \n    discriminator = build_discriminator(config)\n    discriminator.summary()\n\n\n\n    content_loss = keras.losses.MeanSquaredError()\n    adversarial_loss = keras.losses.BinaryCrossentropy()\n    \n    gen_optimizer = keras.optimizers.Adam(learning_rate=config.gen_lr, beta_1=config.beta1)\n    disc_optimizer = keras.optimizers.Adam(learning_rate=config.disc_lr, beta_1=config.beta1)\n\n\n\n    vgg = keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(config.hr_height, config.hr_width, 3))\n    vgg.trainable = False\n    content_model = keras.Model(vgg.input, vgg.get_layer('block5_conv2').output)\n    \n    gen_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n    disc_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n    \n    bce = keras.losses.BinaryCrossentropy()\n    mse = keras.losses.MeanSquaredError()\n    \n    def content_loss(hr, sr):\n        hr_features = content_model(hr)\n        sr_features = content_model(sr)\n        return mse(hr_features, sr_features)\n    \n    def generator_loss(sr_out, hr, sr_validity):\n        con_loss = content_loss(hr, sr_out)\n        adv_loss = bce(tf.ones_like(sr_validity), sr_validity)\n        return con_loss + 1e-3 * adv_loss\n    \n    def discriminator_loss(hr_validity, sr_validity):\n        real_loss = bce(tf.ones_like(hr_validity), hr_validity)\n        fake_loss = bce(tf.zeros_like(sr_validity), sr_validity)\n        return 0.5 * (real_loss + fake_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:09:14.754598Z","iopub.execute_input":"2025-03-19T00:09:14.754927Z","iopub.status.idle":"2025-03-19T00:09:17.192840Z","shell.execute_reply.started":"2025-03-19T00:09:14.754901Z","shell.execute_reply":"2025-03-19T00:09:17.191880Z"}},"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n                                                                                                  \n conv2d (Conv2D)             (None, 64, 64, 64)           15616     ['input_1[0][0]']             \n                                                                                                  \n p_re_lu (PReLU)             (None, 64, 64, 64)           64        ['conv2d[0][0]']              \n                                                                                                  \n conv2d_1 (Conv2D)           (None, 64, 64, 64)           36928     ['p_re_lu[0][0]']             \n                                                                                                  \n batch_normalization (Batch  (None, 64, 64, 64)           256       ['conv2d_1[0][0]']            \n Normalization)                                                                                   \n                                                                                                  \n p_re_lu_1 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization[0][0]'] \n                                                                                                  \n conv2d_2 (Conv2D)           (None, 64, 64, 64)           36928     ['p_re_lu_1[0][0]']           \n                                                                                                  \n batch_normalization_1 (Bat  (None, 64, 64, 64)           256       ['conv2d_2[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n add (Add)                   (None, 64, 64, 64)           0         ['p_re_lu[0][0]',             \n                                                                     'batch_normalization_1[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 64, 64, 64)           36928     ['add[0][0]']                 \n                                                                                                  \n batch_normalization_2 (Bat  (None, 64, 64, 64)           256       ['conv2d_3[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n p_re_lu_2 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_2[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 64, 64, 64)           36928     ['p_re_lu_2[0][0]']           \n                                                                                                  \n batch_normalization_3 (Bat  (None, 64, 64, 64)           256       ['conv2d_4[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n add_1 (Add)                 (None, 64, 64, 64)           0         ['add[0][0]',                 \n                                                                     'batch_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 64, 64, 64)           36928     ['add_1[0][0]']               \n                                                                                                  \n batch_normalization_4 (Bat  (None, 64, 64, 64)           256       ['conv2d_5[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n p_re_lu_3 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_6 (Conv2D)           (None, 64, 64, 64)           36928     ['p_re_lu_3[0][0]']           \n                                                                                                  \n batch_normalization_5 (Bat  (None, 64, 64, 64)           256       ['conv2d_6[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n add_2 (Add)                 (None, 64, 64, 64)           0         ['add_1[0][0]',               \n                                                                     'batch_normalization_5[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_7 (Conv2D)           (None, 64, 64, 64)           36928     ['add_2[0][0]']               \n                                                                                                  \n batch_normalization_6 (Bat  (None, 64, 64, 64)           256       ['conv2d_7[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n p_re_lu_4 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_6[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_8 (Conv2D)           (None, 64, 64, 64)           36928     ['p_re_lu_4[0][0]']           \n                                                                                                  \n batch_normalization_7 (Bat  (None, 64, 64, 64)           256       ['conv2d_8[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n add_3 (Add)                 (None, 64, 64, 64)           0         ['add_2[0][0]',               \n                                                                     'batch_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_9 (Conv2D)           (None, 64, 64, 64)           36928     ['add_3[0][0]']               \n                                                                                                  \n batch_normalization_8 (Bat  (None, 64, 64, 64)           256       ['conv2d_9[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n p_re_lu_5 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_8[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_10 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_5[0][0]']           \n                                                                                                  \n batch_normalization_9 (Bat  (None, 64, 64, 64)           256       ['conv2d_10[0][0]']           \n chNormalization)                                                                                 \n                                                                                                  \n add_4 (Add)                 (None, 64, 64, 64)           0         ['add_3[0][0]',               \n                                                                     'batch_normalization_9[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_11 (Conv2D)          (None, 64, 64, 64)           36928     ['add_4[0][0]']               \n                                                                                                  \n batch_normalization_10 (Ba  (None, 64, 64, 64)           256       ['conv2d_11[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_6 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_10[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_12 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_6[0][0]']           \n                                                                                                  \n batch_normalization_11 (Ba  (None, 64, 64, 64)           256       ['conv2d_12[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_5 (Add)                 (None, 64, 64, 64)           0         ['add_4[0][0]',               \n                                                                     'batch_normalization_11[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_13 (Conv2D)          (None, 64, 64, 64)           36928     ['add_5[0][0]']               \n                                                                                                  \n batch_normalization_12 (Ba  (None, 64, 64, 64)           256       ['conv2d_13[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_7 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_12[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_14 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_7[0][0]']           \n                                                                                                  \n batch_normalization_13 (Ba  (None, 64, 64, 64)           256       ['conv2d_14[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_6 (Add)                 (None, 64, 64, 64)           0         ['add_5[0][0]',               \n                                                                     'batch_normalization_13[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_15 (Conv2D)          (None, 64, 64, 64)           36928     ['add_6[0][0]']               \n                                                                                                  \n batch_normalization_14 (Ba  (None, 64, 64, 64)           256       ['conv2d_15[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_8 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_14[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_16 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_8[0][0]']           \n                                                                                                  \n batch_normalization_15 (Ba  (None, 64, 64, 64)           256       ['conv2d_16[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_7 (Add)                 (None, 64, 64, 64)           0         ['add_6[0][0]',               \n                                                                     'batch_normalization_15[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_17 (Conv2D)          (None, 64, 64, 64)           36928     ['add_7[0][0]']               \n                                                                                                  \n batch_normalization_16 (Ba  (None, 64, 64, 64)           256       ['conv2d_17[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_9 (PReLU)           (None, 64, 64, 64)           64        ['batch_normalization_16[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_18 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_9[0][0]']           \n                                                                                                  \n batch_normalization_17 (Ba  (None, 64, 64, 64)           256       ['conv2d_18[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_8 (Add)                 (None, 64, 64, 64)           0         ['add_7[0][0]',               \n                                                                     'batch_normalization_17[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_19 (Conv2D)          (None, 64, 64, 64)           36928     ['add_8[0][0]']               \n                                                                                                  \n batch_normalization_18 (Ba  (None, 64, 64, 64)           256       ['conv2d_19[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_10 (PReLU)          (None, 64, 64, 64)           64        ['batch_normalization_18[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_20 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_10[0][0]']          \n                                                                                                  \n batch_normalization_19 (Ba  (None, 64, 64, 64)           256       ['conv2d_20[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_9 (Add)                 (None, 64, 64, 64)           0         ['add_8[0][0]',               \n                                                                     'batch_normalization_19[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_21 (Conv2D)          (None, 64, 64, 64)           36928     ['add_9[0][0]']               \n                                                                                                  \n batch_normalization_20 (Ba  (None, 64, 64, 64)           256       ['conv2d_21[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_11 (PReLU)          (None, 64, 64, 64)           64        ['batch_normalization_20[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_22 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_11[0][0]']          \n                                                                                                  \n batch_normalization_21 (Ba  (None, 64, 64, 64)           256       ['conv2d_22[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_10 (Add)                (None, 64, 64, 64)           0         ['add_9[0][0]',               \n                                                                     'batch_normalization_21[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_23 (Conv2D)          (None, 64, 64, 64)           36928     ['add_10[0][0]']              \n                                                                                                  \n batch_normalization_22 (Ba  (None, 64, 64, 64)           256       ['conv2d_23[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_12 (PReLU)          (None, 64, 64, 64)           64        ['batch_normalization_22[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_24 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_12[0][0]']          \n                                                                                                  \n batch_normalization_23 (Ba  (None, 64, 64, 64)           256       ['conv2d_24[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_11 (Add)                (None, 64, 64, 64)           0         ['add_10[0][0]',              \n                                                                     'batch_normalization_23[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_25 (Conv2D)          (None, 64, 64, 64)           36928     ['add_11[0][0]']              \n                                                                                                  \n batch_normalization_24 (Ba  (None, 64, 64, 64)           256       ['conv2d_25[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_13 (PReLU)          (None, 64, 64, 64)           64        ['batch_normalization_24[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_26 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_13[0][0]']          \n                                                                                                  \n batch_normalization_25 (Ba  (None, 64, 64, 64)           256       ['conv2d_26[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_12 (Add)                (None, 64, 64, 64)           0         ['add_11[0][0]',              \n                                                                     'batch_normalization_25[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_27 (Conv2D)          (None, 64, 64, 64)           36928     ['add_12[0][0]']              \n                                                                                                  \n batch_normalization_26 (Ba  (None, 64, 64, 64)           256       ['conv2d_27[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_14 (PReLU)          (None, 64, 64, 64)           64        ['batch_normalization_26[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_28 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_14[0][0]']          \n                                                                                                  \n batch_normalization_27 (Ba  (None, 64, 64, 64)           256       ['conv2d_28[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_13 (Add)                (None, 64, 64, 64)           0         ['add_12[0][0]',              \n                                                                     'batch_normalization_27[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_29 (Conv2D)          (None, 64, 64, 64)           36928     ['add_13[0][0]']              \n                                                                                                  \n batch_normalization_28 (Ba  (None, 64, 64, 64)           256       ['conv2d_29[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_15 (PReLU)          (None, 64, 64, 64)           64        ['batch_normalization_28[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_30 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_15[0][0]']          \n                                                                                                  \n batch_normalization_29 (Ba  (None, 64, 64, 64)           256       ['conv2d_30[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_14 (Add)                (None, 64, 64, 64)           0         ['add_13[0][0]',              \n                                                                     'batch_normalization_29[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_31 (Conv2D)          (None, 64, 64, 64)           36928     ['add_14[0][0]']              \n                                                                                                  \n batch_normalization_30 (Ba  (None, 64, 64, 64)           256       ['conv2d_31[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n p_re_lu_16 (PReLU)          (None, 64, 64, 64)           64        ['batch_normalization_30[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_32 (Conv2D)          (None, 64, 64, 64)           36928     ['p_re_lu_16[0][0]']          \n                                                                                                  \n batch_normalization_31 (Ba  (None, 64, 64, 64)           256       ['conv2d_32[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_15 (Add)                (None, 64, 64, 64)           0         ['add_14[0][0]',              \n                                                                     'batch_normalization_31[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_33 (Conv2D)          (None, 64, 64, 64)           36928     ['add_15[0][0]']              \n                                                                                                  \n batch_normalization_32 (Ba  (None, 64, 64, 64)           256       ['conv2d_33[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n add_16 (Add)                (None, 64, 64, 64)           0         ['p_re_lu[0][0]',             \n                                                                     'batch_normalization_32[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_34 (Conv2D)          (None, 64, 64, 256)          147712    ['add_16[0][0]']              \n                                                                                                  \n lambda (Lambda)             (None, 128, 128, 64)         0         ['conv2d_34[0][0]']           \n                                                                                                  \n p_re_lu_17 (PReLU)          (None, 128, 128, 64)         64        ['lambda[0][0]']              \n                                                                                                  \n conv2d_35 (Conv2D)          (None, 128, 128, 256)        147712    ['p_re_lu_17[0][0]']          \n                                                                                                  \n lambda_1 (Lambda)           (None, 256, 256, 64)         0         ['conv2d_35[0][0]']           \n                                                                                                  \n p_re_lu_18 (PReLU)          (None, 256, 256, 64)         64        ['lambda_1[0][0]']            \n                                                                                                  \n conv2d_36 (Conv2D)          (None, 256, 256, 3)          15555     ['p_re_lu_18[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 1554883 (5.93 MB)\nTrainable params: 1550659 (5.92 MB)\nNon-trainable params: 4224 (16.50 KB)\n__________________________________________________________________________________________________\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n                                                                 \n conv2d_37 (Conv2D)          (None, 256, 256, 64)      1792      \n                                                                 \n leaky_re_lu (LeakyReLU)     (None, 256, 256, 64)      0         \n                                                                 \n conv2d_38 (Conv2D)          (None, 128, 128, 64)      36928     \n                                                                 \n batch_normalization_33 (Ba  (None, 128, 128, 64)      256       \n tchNormalization)                                               \n                                                                 \n leaky_re_lu_1 (LeakyReLU)   (None, 128, 128, 64)      0         \n                                                                 \n conv2d_39 (Conv2D)          (None, 128, 128, 128)     73856     \n                                                                 \n batch_normalization_34 (Ba  (None, 128, 128, 128)     512       \n tchNormalization)                                               \n                                                                 \n leaky_re_lu_2 (LeakyReLU)   (None, 128, 128, 128)     0         \n                                                                 \n conv2d_40 (Conv2D)          (None, 64, 64, 256)       295168    \n                                                                 \n batch_normalization_35 (Ba  (None, 64, 64, 256)       1024      \n tchNormalization)                                               \n                                                                 \n leaky_re_lu_3 (LeakyReLU)   (None, 64, 64, 256)       0         \n                                                                 \n conv2d_41 (Conv2D)          (None, 64, 64, 512)       1180160   \n                                                                 \n batch_normalization_36 (Ba  (None, 64, 64, 512)       2048      \n tchNormalization)                                               \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 64, 64, 512)       0         \n                                                                 \n global_average_pooling2d (  (None, 512)               0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dense (Dense)               (None, 1024)              525312    \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 1024)              0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 1025      \n                                                                 \n=================================================================\nTotal params: 2118081 (8.08 MB)\nTrainable params: 2116161 (8.07 MB)\nNon-trainable params: 1920 (7.50 KB)\n_________________________________________________________________\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 1s 0us/step\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"@tf.function\ndef train_step(lr_images, hr_images):\n    # Ensure inputs are properly normalized to [0,1] range\n    lr_images = tf.clip_by_value(lr_images, 0, 1)\n    hr_images = tf.clip_by_value(hr_images, 0, 1)\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generate SR images\n        sr_images = generator(lr_images, training=True)\n        \n        # Make sure SR images are in valid range\n        sr_images = tf.clip_by_value(sr_images, 0, 1)\n        \n        # Get discriminator outputs\n        hr_validity = discriminator(hr_images, training=True)\n        sr_validity = discriminator(sr_images, training=True)\n        \n        # Calculate VGG feature maps for perceptual loss\n        sr_vgg_features = content_model(sr_images)\n        hr_vgg_features = content_model(hr_images)\n        \n        # Calculate losses\n        # Content loss (pixel + perceptual)\n        pixel_loss = mse(hr_images, sr_images)\n        perceptual_loss = mse(hr_vgg_features, sr_vgg_features)\n        content_l = pixel_loss + 0.006 * perceptual_loss\n        \n        # Adversarial loss\n        adversarial_l = bce(tf.ones_like(sr_validity), sr_validity)\n        \n        # Total generator loss\n        gen_loss = content_l + 1e-3 * adversarial_l\n        \n        # Discriminator loss\n        real_loss = bce(tf.ones_like(hr_validity), hr_validity)\n        fake_loss = bce(tf.zeros_like(sr_validity), sr_validity)\n        disc_loss = 0.5 * (real_loss + fake_loss)\n\n    # Calculate gradients\n    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    # Apply gradients with gradient clipping for stability\n    gen_gradients = [tf.clip_by_norm(g, 1.0) for g in gen_gradients if g is not None]\n    disc_gradients = [tf.clip_by_norm(g, 1.0) for g in disc_gradients if g is not None]\n    \n    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n\n    # For logging, also return content and perceptual losses\n    return content_l, adversarial_l, disc_loss\n\ndef generate_and_save_images(model, epoch, val_dataset):\n    for lr_images, hr_images in val_dataset.take(1):\n        # Generate SR images\n        sr_images = model(lr_images, training=False)\n        \n        # Calculate PSNR for evaluation\n        psnr = tf.reduce_mean(tf.image.psnr(hr_images, sr_images, max_val=1.0))\n        \n        # Create visualization\n        plt.figure(figsize=(15, 8))\n        display_list = [lr_images[0], sr_images[0], hr_images[0]]\n        titles = ['Low Resolution', f'Super Resolution\\nPSNR: {psnr:.2f}', 'High Resolution']\n\n        for i in range(3):\n            plt.subplot(1, 3, i + 1)\n            plt.title(titles[i])\n            plt.imshow(tf.clip_by_value(display_list[i], 0, 1).numpy())\n            plt.axis('off')\n\n        plt.tight_layout()\n        save_path = os.path.join(config.sample_dir, f'image_at_epoch_{epoch:04d}.png')\n        plt.savefig(save_path)\n        print(f\"Saved generated image at: {save_path} (PSNR: {psnr:.2f})\")\n        \n        # Display the figure in the notebook\n        plt.show()\n        plt.close()\n        \n        # Also save individual images for easier inspection\n        single_image_path = os.path.join(config.sample_dir, f'sr_image_epoch_{epoch:04d}.png')\n        plt.figure(figsize=(8, 8))\n        plt.imshow(tf.clip_by_value(sr_images[0], 0, 1).numpy())\n        plt.axis('off')\n        plt.savefig(single_image_path)\n        plt.close()\n\ndef train(dataset, val_dataset, epochs):\n    # Create log for tracking metrics\n    log_file = os.path.join(config.sample_dir, 'training_log.csv')\n    with open(log_file, 'w') as f:\n        f.write('epoch,content_loss,adversarial_loss,disc_loss,psnr\\n')\n    \n    best_psnr = 0\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        pbar = tqdm(total=len(dataset))\n        \n        # Track metrics for this epoch\n        epoch_content_loss = []\n        epoch_adv_loss = []\n        epoch_disc_loss = []\n\n        for lr_images, hr_images in dataset:\n            # Train step\n            content_l, adv_l, disc_l = train_step(lr_images, hr_images)\n            \n            # Update progress bar\n            pbar.update(1)\n            pbar.set_postfix({\n                'content_loss': float(content_l),\n                'adv_loss': float(adv_l),\n                'disc_loss': float(disc_l)\n            })\n            \n            # Track metrics\n            epoch_content_loss.append(float(content_l))\n            epoch_adv_loss.append(float(adv_l))\n            epoch_disc_loss.append(float(disc_l))\n\n        pbar.close()\n        \n        # Calculate average metrics for this epoch\n        avg_content_loss = sum(epoch_content_loss) / len(epoch_content_loss)\n        avg_adv_loss = sum(epoch_adv_loss) / len(epoch_adv_loss)\n        avg_disc_loss = sum(epoch_disc_loss) / len(epoch_disc_loss)\n        \n        # Generate and save images every 5 epochs or at the end\n        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n            # Evaluate PSNR on validation set\n            psnr_values = []\n            for val_lr, val_hr in val_dataset.take(5):  # Evaluate on 5 validation batches\n                val_sr = generator(val_lr, training=False)\n                batch_psnr = tf.reduce_mean(tf.image.psnr(val_hr, val_sr, max_val=1.0))\n                psnr_values.append(float(batch_psnr))\n            \n            avg_psnr = sum(psnr_values) / len(psnr_values)\n            print(f\"Validation PSNR: {avg_psnr:.2f}\")\n            \n            # Save checkpoint if improved\n            if avg_psnr > best_psnr:\n                best_psnr = avg_psnr\n                # Save model weights\n                generator.save_weights(os.path.join(config.checkpoint_dir, 'generator_best.h5'))\n                discriminator.save_weights(os.path.join(config.checkpoint_dir, 'discriminator_best.h5'))\n                print(f\"New best PSNR: {best_psnr:.2f}, saved model\")\n            \n            # Generate and save example images\n            generate_and_save_images(generator, epoch + 1, val_dataset)\n            \n            # Log metrics\n            with open(log_file, 'a') as f:\n                f.write(f\"{epoch+1},{avg_content_loss},{avg_adv_loss},{avg_disc_loss},{avg_psnr}\\n\")\n        \n        # Print epoch summary\n        print(f\"Epoch {epoch+1} summary:\")\n        print(f\"Content Loss: {avg_content_loss:.6f}\")\n        print(f\"Adversarial Loss: {avg_adv_loss:.6f}\")\n        print(f\"Discriminator Loss: {avg_disc_loss:.6f}\")\n\n# Start training with 200 epochs\n# Make sure config.epochs is set to 200 in your SRGANConfig class\nconfig.epochs = 200\ntrain(train_dataset, val_dataset, config.epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T00:09:26.831146Z","iopub.execute_input":"2025-03-19T00:09:26.831500Z","iopub.status.idle":"2025-03-19T02:07:31.339480Z","shell.execute_reply.started":"2025-03-19T00:09:26.831473Z","shell.execute_reply":"2025-03-19T02:07:31.338428Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:51<00:00,  2.23s/it, content_loss=0.0185, adv_loss=1.17, disc_loss=0.379] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 summary:\nContent Loss: 0.039562\nAdversarial Loss: 1.150424\nDiscriminator Loss: 0.443905\n\nEpoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:10<00:00,  1.40s/it, content_loss=0.0169, adv_loss=0.804, disc_loss=0.603]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 summary:\nContent Loss: 0.019536\nAdversarial Loss: 1.366836\nDiscriminator Loss: 0.438012\n\nEpoch 3/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.015, adv_loss=1.28, disc_loss=0.605]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 summary:\nContent Loss: 0.015222\nAdversarial Loss: 0.997447\nDiscriminator Loss: 0.664228\n\nEpoch 4/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:08<00:00,  1.38s/it, content_loss=0.0119, adv_loss=1.46, disc_loss=0.441] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 summary:\nContent Loss: 0.014175\nAdversarial Loss: 0.933514\nDiscriminator Loss: 0.582944\n\nEpoch 5/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0113, adv_loss=1.59, disc_loss=0.347] \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 16.10\nNew best PSNR: 16.10, saved model\nSaved generated image at: samples/image_at_epoch_0005.png (PSNR: 16.41)\nEpoch 5 summary:\nContent Loss: 0.013276\nAdversarial Loss: 1.548811\nDiscriminator Loss: 0.364412\n\nEpoch 6/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.015, adv_loss=2.02, disc_loss=0.19]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 summary:\nContent Loss: 0.013234\nAdversarial Loss: 1.935891\nDiscriminator Loss: 0.321105\n\nEpoch 7/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:08<00:00,  1.38s/it, content_loss=0.00978, adv_loss=3.31, disc_loss=1.01]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 summary:\nContent Loss: 0.012686\nAdversarial Loss: 2.049925\nDiscriminator Loss: 0.371615\n\nEpoch 8/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:08<00:00,  1.38s/it, content_loss=0.00743, adv_loss=3.44, disc_loss=0.684] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 summary:\nContent Loss: 0.011285\nAdversarial Loss: 2.090941\nDiscriminator Loss: 0.538944\n\nEpoch 9/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0088, adv_loss=1.7, disc_loss=0.285]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 summary:\nContent Loss: 0.011707\nAdversarial Loss: 1.860932\nDiscriminator Loss: 0.490661\n\nEpoch 10/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00866, adv_loss=2.75, disc_loss=0.3]  \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 21.38\nNew best PSNR: 21.38, saved model\nSaved generated image at: samples/image_at_epoch_0010.png (PSNR: 21.64)\nEpoch 10 summary:\nContent Loss: 0.011325\nAdversarial Loss: 2.096736\nDiscriminator Loss: 0.353430\n\nEpoch 11/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0109, adv_loss=2.19, disc_loss=0.202]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 summary:\nContent Loss: 0.011206\nAdversarial Loss: 2.346139\nDiscriminator Loss: 0.320898\n\nEpoch 12/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:08<00:00,  1.38s/it, content_loss=0.00905, adv_loss=3.41, disc_loss=0.151]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 summary:\nContent Loss: 0.011229\nAdversarial Loss: 2.207509\nDiscriminator Loss: 0.443440\n\nEpoch 13/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0143, adv_loss=1.59, disc_loss=0.314]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 summary:\nContent Loss: 0.011024\nAdversarial Loss: 2.126788\nDiscriminator Loss: 0.512426\n\nEpoch 14/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.011, adv_loss=3.88, disc_loss=0.453]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 summary:\nContent Loss: 0.010890\nAdversarial Loss: 2.481634\nDiscriminator Loss: 0.268305\n\nEpoch 15/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0132, adv_loss=1.53, disc_loss=0.318] \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 21.89\nNew best PSNR: 21.89, saved model\nSaved generated image at: samples/image_at_epoch_0015.png (PSNR: 22.12)\nEpoch 15 summary:\nContent Loss: 0.011057\nAdversarial Loss: 2.079648\nDiscriminator Loss: 0.404232\n\nEpoch 16/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0138, adv_loss=3.88, disc_loss=0.065]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 summary:\nContent Loss: 0.010959\nAdversarial Loss: 2.167568\nDiscriminator Loss: 0.361412\n\nEpoch 17/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:08<00:00,  1.38s/it, content_loss=0.0096, adv_loss=2.67, disc_loss=0.612]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 summary:\nContent Loss: 0.011025\nAdversarial Loss: 2.063091\nDiscriminator Loss: 0.541251\n\nEpoch 18/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0119, adv_loss=2.58, disc_loss=0.188] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 summary:\nContent Loss: 0.010557\nAdversarial Loss: 2.160192\nDiscriminator Loss: 0.401112\n\nEpoch 19/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0116, adv_loss=1.54, disc_loss=0.387] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 summary:\nContent Loss: 0.011669\nAdversarial Loss: 1.722635\nDiscriminator Loss: 0.543686\n\nEpoch 20/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.01, adv_loss=1.5, disc_loss=0.551]     \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 21.46\nSaved generated image at: samples/image_at_epoch_0020.png (PSNR: 21.64)\nEpoch 20 summary:\nContent Loss: 0.010781\nAdversarial Loss: 1.835399\nDiscriminator Loss: 0.467591\n\nEpoch 21/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00644, adv_loss=2.81, disc_loss=1.29]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 summary:\nContent Loss: 0.010127\nAdversarial Loss: 1.516697\nDiscriminator Loss: 0.636618\n\nEpoch 22/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0136, adv_loss=0.883, disc_loss=0.384]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 summary:\nContent Loss: 0.010320\nAdversarial Loss: 1.609587\nDiscriminator Loss: 0.511863\n\nEpoch 23/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00932, adv_loss=3.65, disc_loss=0.125] \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 23.23\nNew best PSNR: 23.23, saved model\nSaved generated image at: samples/image_at_epoch_0025.png (PSNR: 23.60)\nEpoch 25 summary:\nContent Loss: 0.009114\nAdversarial Loss: 2.151417\nDiscriminator Loss: 0.380769\n\nEpoch 26/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0128, adv_loss=2.19, disc_loss=0.436]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 summary:\nContent Loss: 0.010200\nAdversarial Loss: 2.291577\nDiscriminator Loss: 0.411222\n\nEpoch 27/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0104, adv_loss=1.09, disc_loss=0.316]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 summary:\nContent Loss: 0.010256\nAdversarial Loss: 1.760717\nDiscriminator Loss: 0.699279\n\nEpoch 28/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0116, adv_loss=2.01, disc_loss=0.201] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 summary:\nContent Loss: 0.010205\nAdversarial Loss: 1.908216\nDiscriminator Loss: 0.388385\n\nEpoch 29/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0096, adv_loss=1.26, disc_loss=0.295] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 summary:\nContent Loss: 0.010152\nAdversarial Loss: 1.803838\nDiscriminator Loss: 0.554139\n\nEpoch 30/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00885, adv_loss=0.422, disc_loss=0.663]\n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 22.80\nSaved generated image at: samples/image_at_epoch_0030.png (PSNR: 23.32)\nEpoch 30 summary:\nContent Loss: 0.010747\nAdversarial Loss: 1.783424\nDiscriminator Loss: 0.580161\n\nEpoch 31/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00888, adv_loss=2.97, disc_loss=0.289] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 31 summary:\nContent Loss: 0.009780\nAdversarial Loss: 1.777982\nDiscriminator Loss: 0.483060\n\nEpoch 32/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00811, adv_loss=3.57, disc_loss=1.65] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 32 summary:\nContent Loss: 0.010300\nAdversarial Loss: 1.945236\nDiscriminator Loss: 0.457543\n\nEpoch 33/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00946, adv_loss=2.27, disc_loss=0.107] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 33 summary:\nContent Loss: 0.009734\nAdversarial Loss: 2.020754\nDiscriminator Loss: 0.485248\n\nEpoch 34/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.0101, adv_loss=2.77, disc_loss=0.0752] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 34 summary:\nContent Loss: 0.010163\nAdversarial Loss: 1.743070\nDiscriminator Loss: 0.503698\n\nEpoch 35/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00766, adv_loss=1.19, disc_loss=0.305]\n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 23.33\nNew best PSNR: 23.33, saved model\nSaved generated image at: samples/image_at_epoch_0035.png (PSNR: 23.75)\nEpoch 35 summary:\nContent Loss: 0.009772\nAdversarial Loss: 1.737397\nDiscriminator Loss: 0.457684\n\nEpoch 36/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00823, adv_loss=2.05, disc_loss=0.495] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 36 summary:\nContent Loss: 0.010209\nAdversarial Loss: 1.395922\nDiscriminator Loss: 0.554473\n\nEpoch 37/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:08<00:00,  1.38s/it, content_loss=0.00812, adv_loss=3.14, disc_loss=0.176] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 37 summary:\nContent Loss: 0.009855\nAdversarial Loss: 1.592756\nDiscriminator Loss: 0.508789\n\nEpoch 38/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00915, adv_loss=2.66, disc_loss=0.528] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 38 summary:\nContent Loss: 0.009911\nAdversarial Loss: 1.870862\nDiscriminator Loss: 0.458339\n\nEpoch 39/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00981, adv_loss=2.4, disc_loss=0.109]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 39 summary:\nContent Loss: 0.010008\nAdversarial Loss: 1.894513\nDiscriminator Loss: 0.388938\n\nEpoch 40/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00927, adv_loss=0.455, disc_loss=0.713]\n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 22.79\nSaved generated image at: samples/image_at_epoch_0040.png (PSNR: 23.11)\nEpoch 40 summary:\nContent Loss: 0.009306\nAdversarial Loss: 1.988463\nDiscriminator Loss: 0.613362\n\nEpoch 41/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00819, adv_loss=3.01, disc_loss=0.46] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 41 summary:\nContent Loss: 0.010089\nAdversarial Loss: 1.796853\nDiscriminator Loss: 0.429082\n\nEpoch 42/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00951, adv_loss=0.989, disc_loss=0.79] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 42 summary:\nContent Loss: 0.009624\nAdversarial Loss: 1.565507\nDiscriminator Loss: 0.480846\n\nEpoch 43/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0111, adv_loss=2.4, disc_loss=0.116]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 43 summary:\nContent Loss: 0.010220\nAdversarial Loss: 1.753342\nDiscriminator Loss: 0.557936\n\nEpoch 44/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0126, adv_loss=0.208, disc_loss=1.04]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 44 summary:\nContent Loss: 0.009902\nAdversarial Loss: 1.735405\nDiscriminator Loss: 0.496479\n\nEpoch 45/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00659, adv_loss=3.46, disc_loss=1.02]  \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 22.88\nSaved generated image at: samples/image_at_epoch_0045.png (PSNR: 22.97)\nEpoch 45 summary:\nContent Loss: 0.009694\nAdversarial Loss: 1.988278\nDiscriminator Loss: 0.554234\n\nEpoch 46/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0113, adv_loss=1.11, disc_loss=0.319]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 46 summary:\nContent Loss: 0.010052\nAdversarial Loss: 1.915806\nDiscriminator Loss: 0.496913\n\nEpoch 47/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0136, adv_loss=2.69, disc_loss=0.153] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 47 summary:\nContent Loss: 0.009823\nAdversarial Loss: 1.590087\nDiscriminator Loss: 0.502584\n\nEpoch 48/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:08<00:00,  1.38s/it, content_loss=0.00646, adv_loss=2.1, disc_loss=0.856]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 48 summary:\nContent Loss: 0.009405\nAdversarial Loss: 1.651613\nDiscriminator Loss: 0.573764\n\nEpoch 49/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0141, adv_loss=0.558, disc_loss=0.49]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 49 summary:\nContent Loss: 0.009408\nAdversarial Loss: 1.407340\nDiscriminator Loss: 0.509373\n\nEpoch 50/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00561, adv_loss=2.09, disc_loss=1.55]  \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 23.32\nSaved generated image at: samples/image_at_epoch_0050.png (PSNR: 23.79)\nEpoch 50 summary:\nContent Loss: 0.009014\nAdversarial Loss: 1.898101\nDiscriminator Loss: 0.547280\n\nEpoch 51/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00876, adv_loss=2.51, disc_loss=0.24]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 51 summary:\nContent Loss: 0.009414\nAdversarial Loss: 1.707500\nDiscriminator Loss: 0.502249\n\nEpoch 52/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00638, adv_loss=2.67, disc_loss=0.161] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 52 summary:\nContent Loss: 0.009000\nAdversarial Loss: 1.814795\nDiscriminator Loss: 0.581081\n\nEpoch 53/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0109, adv_loss=1.76, disc_loss=0.279]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 53 summary:\nContent Loss: 0.009815\nAdversarial Loss: 1.536968\nDiscriminator Loss: 0.479381\n\nEpoch 54/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00665, adv_loss=2.66, disc_loss=0.433] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 54 summary:\nContent Loss: 0.009159\nAdversarial Loss: 1.406318\nDiscriminator Loss: 0.547248\n\nEpoch 55/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0087, adv_loss=0.492, disc_loss=0.676] \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 23.81\nNew best PSNR: 23.81, saved model\nSaved generated image at: samples/image_at_epoch_0055.png (PSNR: 24.22)\nEpoch 55 summary:\nContent Loss: 0.009038\nAdversarial Loss: 1.335122\nDiscriminator Loss: 0.547509\n\nEpoch 56/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.0104, adv_loss=1.5, disc_loss=0.165]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 56 summary:\nContent Loss: 0.009059\nAdversarial Loss: 1.449023\nDiscriminator Loss: 0.460493\n\nEpoch 57/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:21<00:00,  1.64s/it, content_loss=0.00828, adv_loss=3.51, disc_loss=0.292] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 57 summary:\nContent Loss: 0.008616\nAdversarial Loss: 1.811204\nDiscriminator Loss: 0.444830\n\nEpoch 58/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:10<00:00,  1.41s/it, content_loss=0.00669, adv_loss=2.4, disc_loss=0.492]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 58 summary:\nContent Loss: 0.008804\nAdversarial Loss: 1.819905\nDiscriminator Loss: 0.438991\n\nEpoch 59/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00776, adv_loss=1.32, disc_loss=0.339]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59 summary:\nContent Loss: 0.009007\nAdversarial Loss: 1.289663\nDiscriminator Loss: 0.608982\n\nEpoch 60/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00925, adv_loss=0.308, disc_loss=0.801]\n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 23.73\nSaved generated image at: samples/image_at_epoch_0060.png (PSNR: 24.26)\nEpoch 60 summary:\nContent Loss: 0.009024\nAdversarial Loss: 1.534677\nDiscriminator Loss: 0.535805\n\nEpoch 61/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0112, adv_loss=1.65, disc_loss=0.253]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 61 summary:\nContent Loss: 0.009186\nAdversarial Loss: 1.501652\nDiscriminator Loss: 0.535117\n\nEpoch 62/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00984, adv_loss=3.37, disc_loss=0.13]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 62 summary:\nContent Loss: 0.008554\nAdversarial Loss: 1.612712\nDiscriminator Loss: 0.498011\n\nEpoch 63/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00854, adv_loss=2.33, disc_loss=0.0909]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63 summary:\nContent Loss: 0.008484\nAdversarial Loss: 1.544616\nDiscriminator Loss: 0.590962\n\nEpoch 64/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00575, adv_loss=1.55, disc_loss=0.188] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 64 summary:\nContent Loss: 0.008285\nAdversarial Loss: 1.731342\nDiscriminator Loss: 0.544279\n\nEpoch 65/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0105, adv_loss=2.09, disc_loss=0.216]  \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 24.47\nNew best PSNR: 24.47, saved model\nSaved generated image at: samples/image_at_epoch_0065.png (PSNR: 24.75)\nEpoch 65 summary:\nContent Loss: 0.008613\nAdversarial Loss: 1.641834\nDiscriminator Loss: 0.471922\n\nEpoch 66/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00676, adv_loss=1.6, disc_loss=0.341]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 66 summary:\nContent Loss: 0.008095\nAdversarial Loss: 1.784030\nDiscriminator Loss: 0.431543\n\nEpoch 67/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0113, adv_loss=1.07, disc_loss=0.402]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 67 summary:\nContent Loss: 0.008827\nAdversarial Loss: 1.677195\nDiscriminator Loss: 0.430961\n\nEpoch 68/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00474, adv_loss=2.08, disc_loss=0.175] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 68 summary:\nContent Loss: 0.008637\nAdversarial Loss: 1.419014\nDiscriminator Loss: 0.536361\n\nEpoch 69/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0108, adv_loss=2.58, disc_loss=0.273]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 69 summary:\nContent Loss: 0.008869\nAdversarial Loss: 1.639896\nDiscriminator Loss: 0.452628\n\nEpoch 70/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00758, adv_loss=1.18, disc_loss=1.28]  \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 16.68\nSaved generated image at: samples/image_at_epoch_0070.png (PSNR: 15.71)\nEpoch 70 summary:\nContent Loss: 0.009214\nAdversarial Loss: 1.516699\nDiscriminator Loss: 0.632290\n\nEpoch 71/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00804, adv_loss=1.09, disc_loss=0.391] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 71 summary:\nContent Loss: 0.008962\nAdversarial Loss: 1.390493\nDiscriminator Loss: 0.558765\n\nEpoch 72/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.0063, adv_loss=3.07, disc_loss=1.01]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 72 summary:\nContent Loss: 0.008506\nAdversarial Loss: 1.936996\nDiscriminator Loss: 0.446400\n\nEpoch 73/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00743, adv_loss=2.46, disc_loss=0.097] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 73 summary:\nContent Loss: 0.008579\nAdversarial Loss: 1.836167\nDiscriminator Loss: 0.479770\n\nEpoch 74/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00967, adv_loss=0.558, disc_loss=0.733]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74 summary:\nContent Loss: 0.009191\nAdversarial Loss: 1.487190\nDiscriminator Loss: 0.572594\n\nEpoch 75/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0104, adv_loss=1.39, disc_loss=0.317]  \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 22.50\nSaved generated image at: samples/image_at_epoch_0075.png (PSNR: 23.08)\nEpoch 75 summary:\nContent Loss: 0.008245\nAdversarial Loss: 1.615277\nDiscriminator Loss: 0.488249\n\nEpoch 76/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00651, adv_loss=1.69, disc_loss=0.256] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 76 summary:\nContent Loss: 0.009148\nAdversarial Loss: 1.519805\nDiscriminator Loss: 0.648498\n\nEpoch 77/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00845, adv_loss=0.631, disc_loss=1.04] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 77 summary:\nContent Loss: 0.008847\nAdversarial Loss: 1.396663\nDiscriminator Loss: 0.525836\n\nEpoch 78/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00936, adv_loss=1.33, disc_loss=1.09]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 78 summary:\nContent Loss: 0.008440\nAdversarial Loss: 1.667346\nDiscriminator Loss: 0.472425\n\nEpoch 79/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00953, adv_loss=0.658, disc_loss=0.448]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79 summary:\nContent Loss: 0.008535\nAdversarial Loss: 1.334868\nDiscriminator Loss: 0.541003\n\nEpoch 80/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00594, adv_loss=1.75, disc_loss=0.853] \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 22.19\nSaved generated image at: samples/image_at_epoch_0080.png (PSNR: 22.52)\nEpoch 80 summary:\nContent Loss: 0.008455\nAdversarial Loss: 1.655616\nDiscriminator Loss: 0.623799\n\nEpoch 81/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00642, adv_loss=2.71, disc_loss=0.965] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 81 summary:\nContent Loss: 0.008298\nAdversarial Loss: 1.384813\nDiscriminator Loss: 0.518188\n\nEpoch 82/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.38s/it, content_loss=0.00814, adv_loss=1.11, disc_loss=0.29]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 82 summary:\nContent Loss: 0.008587\nAdversarial Loss: 1.419652\nDiscriminator Loss: 0.514972\n\nEpoch 83/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00935, adv_loss=2.91, disc_loss=0.404] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 83 summary:\nContent Loss: 0.008669\nAdversarial Loss: 1.347724\nDiscriminator Loss: 0.556552\n\nEpoch 84/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00785, adv_loss=1.19, disc_loss=0.288] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 84 summary:\nContent Loss: 0.008822\nAdversarial Loss: 1.349598\nDiscriminator Loss: 0.528966\n\nEpoch 85/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00446, adv_loss=1.53, disc_loss=0.272] \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 24.99\nNew best PSNR: 24.99, saved model\nSaved generated image at: samples/image_at_epoch_0085.png (PSNR: 25.03)\nEpoch 85 summary:\nContent Loss: 0.007421\nAdversarial Loss: 1.530020\nDiscriminator Loss: 0.451714\n\nEpoch 86/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00538, adv_loss=0.986, disc_loss=0.668]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 86 summary:\nContent Loss: 0.008827\nAdversarial Loss: 1.331366\nDiscriminator Loss: 0.641704\n\nEpoch 87/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00782, adv_loss=0.423, disc_loss=0.857]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 87 summary:\nContent Loss: 0.008101\nAdversarial Loss: 1.737341\nDiscriminator Loss: 0.427706\n\nEpoch 88/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00486, adv_loss=2.22, disc_loss=0.812] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 88 summary:\nContent Loss: 0.008068\nAdversarial Loss: 1.384636\nDiscriminator Loss: 0.534924\n\nEpoch 89/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0111, adv_loss=1, disc_loss=0.331]     \n","output_type":"stream"},{"name":"stdout","text":"Epoch 89 summary:\nContent Loss: 0.008263\nAdversarial Loss: 1.314487\nDiscriminator Loss: 0.545404\n\nEpoch 90/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00543, adv_loss=2.29, disc_loss=0.717] \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 24.16\nSaved generated image at: samples/image_at_epoch_0090.png (PSNR: 24.87)\nEpoch 90 summary:\nContent Loss: 0.008224\nAdversarial Loss: 1.529241\nDiscriminator Loss: 0.487912\n\nEpoch 91/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00829, adv_loss=0.872, disc_loss=0.367]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 91 summary:\nContent Loss: 0.008015\nAdversarial Loss: 1.370349\nDiscriminator Loss: 0.517211\n\nEpoch 92/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00691, adv_loss=1.41, disc_loss=0.356] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 92 summary:\nContent Loss: 0.008744\nAdversarial Loss: 1.384454\nDiscriminator Loss: 0.549610\n\nEpoch 93/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0053, adv_loss=2.14, disc_loss=0.18]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 93 summary:\nContent Loss: 0.008162\nAdversarial Loss: 1.548895\nDiscriminator Loss: 0.453954\n\nEpoch 94/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.0064, adv_loss=0.841, disc_loss=1.51]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 94 summary:\nContent Loss: 0.008139\nAdversarial Loss: 1.334569\nDiscriminator Loss: 0.542012\n\nEpoch 95/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00927, adv_loss=1.03, disc_loss=0.32]  \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 23.68\nSaved generated image at: samples/image_at_epoch_0095.png (PSNR: 24.22)\nEpoch 95 summary:\nContent Loss: 0.008585\nAdversarial Loss: 1.301029\nDiscriminator Loss: 0.514409\n\nEpoch 96/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00719, adv_loss=1.6, disc_loss=0.498]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 96 summary:\nContent Loss: 0.007849\nAdversarial Loss: 1.489700\nDiscriminator Loss: 0.512466\n\nEpoch 97/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00899, adv_loss=0.512, disc_loss=1.15] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 97 summary:\nContent Loss: 0.008612\nAdversarial Loss: 1.381944\nDiscriminator Loss: 0.520936\n\nEpoch 98/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.00512, adv_loss=2.37, disc_loss=0.934] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 98 summary:\nContent Loss: 0.008284\nAdversarial Loss: 1.452028\nDiscriminator Loss: 0.553773\n\nEpoch 99/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.40s/it, content_loss=0.0055, adv_loss=0.86, disc_loss=0.394]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 99 summary:\nContent Loss: 0.007900\nAdversarial Loss: 1.459829\nDiscriminator Loss: 0.557941\n\nEpoch 100/100\n","output_type":"stream"},{"name":"stderr","text":"100%|| 50/50 [01:09<00:00,  1.39s/it, content_loss=0.00651, adv_loss=1, disc_loss=0.475]    \n","output_type":"stream"},{"name":"stdout","text":"Validation PSNR: 24.82\nSaved generated image at: samples/image_at_epoch_0100.png (PSNR: 25.26)\nEpoch 100 summary:\nContent Loss: 0.007770\nAdversarial Loss: 1.382867\nDiscriminator Loss: 0.517238\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load the best weights from previous training\ntry:\n    generator.load_weights(os.path.join(config.checkpoint_dir, 'generator_best.h5'))\n    print(\"Loaded generator weights successfully\")\n    \n    # Try to load discriminator weights if they exist\n    try:\n        discriminator.load_weights(os.path.join(config.checkpoint_dir, 'discriminator_best.h5'))\n        print(\"Loaded discriminator weights successfully\")\n    except:\n        print(\"Could not load discriminator weights, starting with fresh discriminator\")\nexcept:\n    print(\"Could not load generator weights, starting with fresh model\")\n\ndef continue_training(dataset, val_dataset, start_epoch, total_epochs):\n    \"\"\"Continue training from a saved checkpoint\"\"\"\n    # Load existing log if available\n    log_file = os.path.join(config.sample_dir, 'training_log.csv')\n    \n    # Find the best PSNR from previous training\n    best_psnr = 24.99  # Based on your logs, best PSNR was 24.99 at epoch 85\n    print(f\"Continuing training from epoch {start_epoch+1} with previous best PSNR: {best_psnr:.2f}\")\n    \n    for epoch in range(start_epoch, total_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{total_epochs}\")\n        pbar = tqdm(total=len(dataset))\n        \n        # Track metrics for this epoch\n        epoch_content_loss = []\n        epoch_adv_loss = []\n        epoch_disc_loss = []\n\n        for lr_images, hr_images in dataset:\n            # Train step\n            content_l, adv_l, disc_l = train_step(lr_images, hr_images)\n            \n            # Update progress bar\n            pbar.update(1)\n            pbar.set_postfix({\n                'content_loss': float(content_l),\n                'adv_loss': float(adv_l),\n                'disc_loss': float(disc_l)\n            })\n            \n            # Track metrics\n            epoch_content_loss.append(float(content_l))\n            epoch_adv_loss.append(float(adv_l))\n            epoch_disc_loss.append(float(disc_l))\n\n        pbar.close()\n        \n        # Calculate average metrics for this epoch\n        avg_content_loss = sum(epoch_content_loss) / len(epoch_content_loss)\n        avg_adv_loss = sum(epoch_adv_loss) / len(epoch_adv_loss)\n        avg_disc_loss = sum(epoch_disc_loss) / len(epoch_disc_loss)\n        \n        # Generate and save images every 5 epochs or at the end\n        if (epoch + 1) % 5 == 0 or epoch == total_epochs - 1:\n            # Evaluate PSNR on validation set\n            psnr_values = []\n            for val_lr, val_hr in val_dataset.take(5):  # Evaluate on 5 validation batches\n                val_sr = generator(val_lr, training=False)\n                batch_psnr = tf.reduce_mean(tf.image.psnr(val_hr, val_sr, max_val=1.0))\n                psnr_values.append(float(batch_psnr))\n            \n            avg_psnr = sum(psnr_values) / len(psnr_values)\n            print(f\"Validation PSNR: {avg_psnr:.2f}\")\n            \n            # Save checkpoint if improved\n            if avg_psnr > best_psnr:\n                best_psnr = avg_psnr\n                # Save model weights\n                generator.save_weights(os.path.join(config.checkpoint_dir, 'generator_best.h5'))\n                discriminator.save_weights(os.path.join(config.checkpoint_dir, 'discriminator_best.h5'))\n                print(f\"New best PSNR: {best_psnr:.2f}, saved model\")\n            \n            # Generate and save example images\n            generate_and_save_images(generator, epoch + 1, val_dataset)\n            \n            # Log metrics\n            with open(log_file, 'a') as f:\n                f.write(f\"{epoch+1},{avg_content_loss},{avg_adv_loss},{avg_disc_loss},{avg_psnr}\\n\")\n        \n        # Print epoch summary\n        print(f\"Epoch {epoch+1} summary:\")\n        print(f\"Content Loss: {avg_content_loss:.6f}\")\n        print(f\"Adversarial Loss: {avg_adv_loss:.6f}\")\n        print(f\"Discriminator Loss: {avg_disc_loss:.6f}\")\n\n# Continue training from epoch 100 to 200\ncontinue_training(train_dataset, val_dataset, start_epoch=100, total_epochs=200)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T18:08:26.170306Z","iopub.execute_input":"2025-03-19T18:08:26.170852Z","iopub.status.idle":"2025-03-19T18:08:26.207842Z","shell.execute_reply.started":"2025-03-19T18:08:26.170821Z","shell.execute_reply":"2025-03-19T18:08:26.206764Z"}},"outputs":[{"name":"stdout","text":"Could not load generator weights, starting with fresh model\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiscriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_disc_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Continue training from epoch 100 to 200\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m continue_training(\u001b[43mtrain_dataset\u001b[49m, val_dataset, start_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, total_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"],"ename":"NameError","evalue":"name 'train_dataset' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}