{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":588358,"sourceType":"datasetVersion","datasetId":286056}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Set CUDA_VISIBLE_DEVICES to restrict TensorFlow to only use the first GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n\n# Verify that TensorFlow sees only one GPU\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    print(f\"Using GPU: {physical_devices[0]}\")\nelse:\n    print(\"No GPU found. Using CPU.\")\n\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nprint(f\"TensorFlow version: {tf.__version__}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-03-20T09:08:20.149393Z","iopub.execute_input":"2025-03-20T09:08:20.149767Z","iopub.status.idle":"2025-03-20T09:08:31.514525Z","shell.execute_reply.started":"2025-03-20T09:08:20.149725Z","shell.execute_reply":"2025-03-20T09:08:31.513755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SRGANConfig:\n    def __init__(self):\n        # Dataset parameters\n        self.dataset_name = \"DIV2K\"\n        self.data_dir = \"../input/div2k-dataset/DIV2K_train_HR/DIV2K_train_HR\"\n        self.val_dir = \"../input/div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\"\n        self.hr_height = 256\n        self.hr_width = 256\n        self.scale_factor = 4\n        self.lr_height = self.hr_height // self.scale_factor\n        self.lr_width = self.hr_width // self.scale_factor\n        \n        # Training parameters\n        self.epochs = 200\n        self.batch_size = 16\n        self.gen_lr = 1e-4\n        self.disc_lr = 1e-4\n        self.beta1 = 0.5\n        \n        # Model parameters\n        self.gen_filters = 64\n        self.disc_filters = 64\n        self.num_res_blocks = 16\n        \n        # Loss weights\n        self.content_weight = 1.0\n        self.adversarial_weight = 1e-3\n        \n        # Paths\n        self.checkpoint_dir = \"checkpoints\"\n        self.sample_dir = \"samples\"\n        \n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        os.makedirs(self.sample_dir, exist_ok=True)\n\nconfig = SRGANConfig()\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T09:08:31.515799Z","iopub.execute_input":"2025-03-20T09:08:31.516276Z","iopub.status.idle":"2025-03-20T09:08:31.523160Z","shell.execute_reply.started":"2025-03-20T09:08:31.516251Z","shell.execute_reply":"2025-03-20T09:08:31.522348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataLoader:\n    def __init__(self, config):\n        self.config = config\n        \n    def _verify_paths(self):\n        # Check if directories exist\n        if not tf.io.gfile.exists(self.config.data_dir):\n            raise ValueError(f\"Training directory {self.config.data_dir} does not exist\")\n        if not tf.io.gfile.exists(self.config.val_dir):\n            raise ValueError(f\"Validation directory {self.config.val_dir} does not exist\")\n            \n        # Check if files exist\n        train_files = tf.io.gfile.glob(os.path.join(self.config.data_dir, \"*.png\"))\n        val_files = tf.io.gfile.glob(os.path.join(self.config.val_dir, \"*.png\"))\n        \n        if not train_files:\n            raise ValueError(f\"No PNG files found in {self.config.data_dir}\")\n        if not val_files:\n            raise ValueError(f\"No PNG files found in {self.config.val_dir}\")\n            \n        return len(train_files), len(val_files)\n    \n    def load_and_preprocess(self, image_path):\n        img = tf.io.read_file(image_path)\n        img = tf.image.decode_png(img, channels=3)\n        img = tf.image.random_crop(img, [self.config.hr_height, self.config.hr_width, 3])\n        img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n        \n        # Create low-res version\n        lr_img = tf.image.resize(img, [self.config.lr_height, self.config.lr_width], \n                               method='bicubic')\n        return lr_img, img\n    \n    def create_dataset(self, is_training=True):\n        # Verify paths and get file counts\n        train_count, val_count = self._verify_paths()\n        \n        # Create dataset\n        if is_training:\n            files = tf.data.Dataset.list_files(os.path.join(self.config.data_dir, \"*.png\"), \n                                             shuffle=True)\n            print(f\"Found {train_count} training images\")\n        else:\n            files = tf.data.Dataset.list_files(os.path.join(self.config.val_dir, \"*.png\"), \n                                             shuffle=False)\n            print(f\"Found {val_count} validation images\")\n            \n        dataset = files.map(self.load_and_preprocess, \n                          num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.batch(self.config.batch_size)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset\n\n# Usage\ndata_loader = DataLoader(config)\ntrain_dataset = data_loader.create_dataset(is_training=True)\nval_dataset = data_loader.create_dataset(is_training=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T09:08:31.524682Z","iopub.execute_input":"2025-03-20T09:08:31.525026Z","iopub.status.idle":"2025-03-20T09:08:32.238724Z","shell.execute_reply.started":"2025-03-20T09:08:31.524993Z","shell.execute_reply":"2025-03-20T09:08:32.237845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n    def residual_block(x, filters):\n        shortcut = x\n        x = layers.Conv2D(filters, 3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.PReLU(shared_axes=[1, 2])(x)\n        x = layers.Conv2D(filters, 3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Add()([shortcut, x])\n        return x\n    \n    def upsample_block(x, filters):\n        x = layers.Conv2D(filters * 4, 3, padding='same')(x)\n        x = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2))(x)\n        x = layers.PReLU(shared_axes=[1, 2])(x)\n        return x\n    \n    def build_generator(config):\n        inputs = layers.Input(shape=(config.lr_height, config.lr_width, 3))\n        x = layers.Conv2D(64, 9, padding='same')(inputs)\n        x = layers.PReLU(shared_axes=[1, 2])(x)\n    \n        shortcut = x\n    \n        for _ in range(16):\n            x = residual_block(x, 64)\n    \n        x = layers.Conv2D(64, 3, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Add()([shortcut, x])\n    \n        for _ in range(2):\n            x = upsample_block(x, 64)\n    \n        outputs = layers.Conv2D(3, 9, padding='same', activation='tanh')(x)\n    \n        return keras.Model(inputs, outputs)\n    \n    generator = build_generator(config)\n    generator.summary()\n\n\n    def build_discriminator(config):\n        inputs = layers.Input(shape=(config.hr_height, config.hr_width, 3))\n        \n        # Initial convolutional layer\n        x = layers.Conv2D(64, kernel_size=3, strides=1, padding=\"same\")(inputs)\n        x = layers.LeakyReLU(alpha=0.2)(x)\n    \n        # Series of convolutional blocks with increasing filters\n        for i in range(4):\n            filters = min(64 * (2 ** i), 512)  # Cap filters at 512\n            strides = 1 if i % 2 else 2       # Alternate strides between 1 and 2\n            x = layers.Conv2D(filters=filters,\n                              kernel_size=3,\n                              strides=strides,\n                              padding=\"same\")(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.LeakyReLU(alpha=0.2)(x)\n    \n        # Fully connected layer for classification\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(1024)(x)\n        x = layers.LeakyReLU(alpha=0.2)(x)\n        \n        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n        return keras.Model(inputs, outputs)\n    \n    discriminator = build_discriminator(config)\n    discriminator.summary()\n\n\n\n    content_loss = keras.losses.MeanSquaredError()\n    adversarial_loss = keras.losses.BinaryCrossentropy()\n    \n    gen_optimizer = keras.optimizers.Adam(learning_rate=config.gen_lr, beta_1=config.beta1)\n    disc_optimizer = keras.optimizers.Adam(learning_rate=config.disc_lr, beta_1=config.beta1)\n\n\n\n    vgg = keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(config.hr_height, config.hr_width, 3))\n    vgg.trainable = False\n    content_model = keras.Model(vgg.input, vgg.get_layer('block5_conv2').output)\n    \n    gen_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n    disc_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n    \n    bce = keras.losses.BinaryCrossentropy()\n    mse = keras.losses.MeanSquaredError()\n    \n    def content_loss(hr, sr):\n        hr_features = content_model(hr)\n        sr_features = content_model(sr)\n        return mse(hr_features, sr_features)\n    \n    def generator_loss(sr_out, hr, sr_validity):\n        con_loss = content_loss(hr, sr_out)\n        adv_loss = bce(tf.ones_like(sr_validity), sr_validity)\n        return con_loss + 1e-3 * adv_loss\n    \n    def discriminator_loss(hr_validity, sr_validity):\n        real_loss = bce(tf.ones_like(hr_validity), hr_validity)\n        fake_loss = bce(tf.zeros_like(sr_validity), sr_validity)\n        return 0.5 * (real_loss + fake_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T09:08:32.240959Z","iopub.execute_input":"2025-03-20T09:08:32.241285Z","iopub.status.idle":"2025-03-20T09:08:36.537138Z","shell.execute_reply.started":"2025-03-20T09:08:32.241254Z","shell.execute_reply":"2025-03-20T09:08:36.536436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef train_step(lr_images, hr_images):\n    # Ensure inputs are properly normalized to [0,1] range\n    lr_images = tf.clip_by_value(lr_images, 0, 1)\n    hr_images = tf.clip_by_value(hr_images, 0, 1)\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generate SR images\n        sr_images = generator(lr_images, training=True)\n        \n        # Make sure SR images are in valid range\n        sr_images = tf.clip_by_value(sr_images, 0, 1)\n        \n        # Get discriminator outputs\n        hr_validity = discriminator(hr_images, training=True)\n        sr_validity = discriminator(sr_images, training=True)\n        \n        # Calculate VGG feature maps for perceptual loss\n        sr_vgg_features = content_model(sr_images)\n        hr_vgg_features = content_model(hr_images)\n        \n        # Calculate losses\n        # Content loss (pixel + perceptual)\n        pixel_loss = mse(hr_images, sr_images)\n        perceptual_loss = mse(hr_vgg_features, sr_vgg_features)\n        content_l = pixel_loss + 0.006 * perceptual_loss\n        \n        # Adversarial loss\n        adversarial_l = bce(tf.ones_like(sr_validity), sr_validity)\n        \n        # Total generator loss\n        gen_loss = content_l + 1e-3 * adversarial_l\n        \n        # Discriminator loss\n        real_loss = bce(tf.ones_like(hr_validity), hr_validity)\n        fake_loss = bce(tf.zeros_like(sr_validity), sr_validity)\n        disc_loss = 0.5 * (real_loss + fake_loss)\n\n    # Calculate gradients\n    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    # Apply gradients with gradient clipping for stability\n    gen_gradients = [tf.clip_by_norm(g, 1.0) for g in gen_gradients if g is not None]\n    disc_gradients = [tf.clip_by_norm(g, 1.0) for g in disc_gradients if g is not None]\n    \n    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n\n    # For logging, also return content and perceptual losses\n    return content_l, adversarial_l, disc_loss\n\ndef generate_and_save_images(model, epoch, val_dataset):\n    # Iterate over one batch from validation set\n    for lr_images, hr_images in val_dataset.take(1):\n        # Generate SR images\n        sr_images = model(lr_images, training=False)\n        \n        # Calculate PSNR for evaluation\n        psnr_val = tf.reduce_mean(tf.image.psnr(hr_images, sr_images, max_val=1.0))\n        \n        # Create visualization (composite image)\n        display_list = [lr_images[0], sr_images[0], hr_images[0]]\n        titles = ['Low Resolution', f'Super Resolution\\nPSNR: {psnr_val:.2f}', 'High Resolution']\n        \n        # Create figure for composite display\n        fig, axes = plt.subplots(1, 3, figsize=(15, 8))\n        for i, ax in enumerate(axes):\n            ax.set_title(titles[i])\n            ax.imshow(tf.clip_by_value(display_list[i], 0, 1).numpy())\n            ax.axis('off')\n        plt.tight_layout()\n        \n        # Save composite image\n        save_path = os.path.join(config.sample_dir, f'image_at_epoch_{epoch:04d}.png')\n        plt.savefig(save_path)\n        print(f\"Saved generated composite image at: {save_path} (PSNR: {psnr_val:.2f})\")\n        \n        # Display the composite image inline\n        plt.show()\n        plt.close(fig)\n        \n        # Save and display individual SR image for easier inspection\n        fig_single, ax_single = plt.subplots(1, 1, figsize=(8, 8))\n        ax_single.imshow(tf.clip_by_value(sr_images[0], 0, 1).numpy())\n        ax_single.axis('off')\n        single_image_path = os.path.join(config.sample_dir, f'sr_image_epoch_{epoch:04d}.png')\n        plt.savefig(single_image_path)\n        print(f\"Saved generated SR image at: {single_image_path}\")\n        \n        # Display the individual SR image inline\n        plt.show()\n        plt.close(fig_single)\n\n\ndef train(dataset, val_dataset, epochs):\n    # Create log for tracking metrics\n    log_file = os.path.join(config.sample_dir, 'training_log.csv')\n    with open(log_file, 'w') as f:\n        f.write('epoch,content_loss,adversarial_loss,disc_loss,psnr\\n')\n    \n    best_psnr = 0\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        pbar = tqdm(total=len(dataset))\n        \n        # Track metrics for this epoch\n        epoch_content_loss = []\n        epoch_adv_loss = []\n        epoch_disc_loss = []\n\n        for lr_images, hr_images in dataset:\n            # Train step\n            content_l, adv_l, disc_l = train_step(lr_images, hr_images)\n            \n            # Update progress bar\n            pbar.update(1)\n            pbar.set_postfix({\n                'content_loss': float(content_l),\n                'adv_loss': float(adv_l),\n                'disc_loss': float(disc_l)\n            })\n            \n            # Track metrics\n            epoch_content_loss.append(float(content_l))\n            epoch_adv_loss.append(float(adv_l))\n            epoch_disc_loss.append(float(disc_l))\n\n        pbar.close()\n        \n        # Calculate average metrics for this epoch\n        avg_content_loss = sum(epoch_content_loss) / len(epoch_content_loss)\n        avg_adv_loss = sum(epoch_adv_loss) / len(epoch_adv_loss)\n        avg_disc_loss = sum(epoch_disc_loss) / len(epoch_disc_loss)\n        \n        # Generate and save images\n        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n            # Evaluate PSNR on validation set\n            psnr_values = []\n            for val_lr, val_hr in val_dataset.take(5):  # Evaluate on 5 validation batches\n                val_sr = generator(val_lr, training=False)\n                batch_psnr = tf.reduce_mean(tf.image.psnr(val_hr, val_sr, max_val=1.0))\n                psnr_values.append(float(batch_psnr))\n            \n            avg_psnr = sum(psnr_values) / len(psnr_values)\n            print(f\"Validation PSNR: {avg_psnr:.2f}\")\n            \n            # Save checkpoint if improved\n            if avg_psnr > best_psnr:\n                best_psnr = avg_psnr\n                # Save model weights\n                generator.save_weights(os.path.join(config.checkpoint_dir, 'generator_best.h5'))\n                print(f\"New best PSNR: {best_psnr:.2f}, saved model\")\n            \n            # Generate and save example images\n            generate_and_save_images(generator, epoch + 1, val_dataset)\n            \n            # Log metrics\n            with open(log_file, 'a') as f:\n                f.write(f\"{epoch+1},{avg_content_loss},{avg_adv_loss},{avg_disc_loss},{avg_psnr}\\n\")\n        \n        # Print epoch summary\n        print(f\"Epoch {epoch+1} summary:\")\n        print(f\"Content Loss: {avg_content_loss:.6f}\")\n        print(f\"Adversarial Loss: {avg_adv_loss:.6f}\")\n        print(f\"Discriminator Loss: {avg_disc_loss:.6f}\")\n\n# Start training with more epochs for better results\ntrain(train_dataset, val_dataset, config.epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T09:08:36.538123Z","iopub.execute_input":"2025-03-20T09:08:36.538366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Path to your saved generator model\nMODEL_PATH = \"checkpoints/generator_best.h5\"\n# Directory containing test images\nTEST_DIR = \"../input/div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\"\n# Directory to save outputs\nOUTPUT_DIR = \"test_results\"\n# Scale factor (4x upscaling)\nSCALE_FACTOR = 4\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Load the trained generator\ndef load_generator(model_path, config):\n    # Recreate the generator architecture\n    generator = build_generator(config)\n    # Load weights\n    generator.load_weights(model_path)\n    return generator\n\n# Function to preprocess a single image\ndef preprocess_image(image_path, target_size=None):\n    # Read image\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_image(img, channels=3)\n    \n    # Ensure image has 3 channels\n    if img.shape[-1] != 3:\n        if img.shape[-1] == 4:  # Handle RGBA\n            img = img[..., :3]\n        else:  # Handle grayscale\n            img = tf.image.grayscale_to_rgb(img)\n    \n    # Convert to float and normalize\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    # Resize if target size provided\n    if target_size:\n        img = tf.image.resize(img, target_size, method='bicubic')\n    \n    # Add batch dimension\n    img = tf.expand_dims(img, 0)\n    return img\n\n# Function to test on a single image\ndef super_resolve_image(model, image_path, output_path=None):\n    # Get filename for display\n    filename = os.path.basename(image_path)\n    \n    # Load and preprocess image\n    lr_img = preprocess_image(image_path)\n    \n    # Get original dimensions for reference\n    original_img = Image.open(image_path)\n    original_width, original_height = original_img.size\n    \n    # Generate super-resolution image\n    sr_img = model(lr_img, training=False)\n    \n    # Convert to numpy arrays and denormalize\n    lr_img = tf.squeeze(lr_img).numpy()\n    sr_img = tf.squeeze(sr_img).numpy()\n    \n    # Clip values to valid range\n    lr_img = np.clip(lr_img, 0, 1)\n    sr_img = np.clip(sr_img, 0, 1)\n    \n    # Calculate upscaled dimensions\n    upscaled_height, upscaled_width = sr_img.shape[:2]\n    \n    # Calculate PSNR (if we had ground truth)\n    # psnr = tf.image.psnr(hr_img, sr_img, max_val=1.0).numpy()\n    \n    # Display results\n    plt.figure(figsize=(16, 8))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(lr_img)\n    plt.title(f\"Low Resolution\\n{original_width}×{original_height}\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(sr_img)\n    plt.title(f\"Super Resolution\\n{upscaled_width}×{upscaled_height}\")\n    plt.axis('off')\n    \n    plt.suptitle(f\"Super-Resolution Result: {filename}\", fontsize=16)\n    plt.tight_layout()\n    \n    # Save figure if output path is provided\n    if output_path:\n        plt.savefig(output_path)\n        print(f\"Saved comparison to: {output_path}\")\n        \n        # Also save just the SR image\n        sr_output_path = os.path.join(os.path.dirname(output_path), f\"sr_{os.path.basename(output_path)}\")\n        sr_img_pil = Image.fromarray((sr_img * 255).astype(np.uint8))\n        sr_img_pil.save(sr_output_path)\n        print(f\"Saved SR image to: {sr_output_path}\")\n    \n    plt.show()\n    \n    return sr_img\n\n# Test on all images in a directory\ndef test_on_directory(model, test_dir, output_dir):\n    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n    test_images = []\n    \n    # Find all images in the directory\n    for file in os.listdir(test_dir):\n        if any(file.lower().endswith(ext) for ext in image_extensions):\n            test_images.append(os.path.join(test_dir, file))\n    \n    if not test_images:\n        print(f\"No images found in {test_dir}\")\n        return\n    \n    print(f\"Found {len(test_images)} images to process\")\n    \n    # Process each image\n    for img_path in test_images:\n        filename = os.path.basename(img_path)\n        output_path = os.path.join(output_dir, f\"comparison_{filename}\")\n        super_resolve_image(model, img_path, output_path)\n        print(f\"Processed: {filename}\")\n        print(\"-\" * 50)\n\n# Load your model and run the test\nif __name__ == \"__main__\":\n    # Recreate your config if needed\n    class Config:\n        def __init__(self):\n            self.lr_height = 64\n            self.lr_width = 64\n            self.scale_factor = 4\n    \n    config = Config()\n    \n    # Load the model\n    generator = load_generator(MODEL_PATH, config)\n    print(\"Model loaded successfully\")\n    \n    # For a single image test\n    # super_resolve_image(generator, \"test_images/sample.jpg\", \"test_results/sample_result.jpg\")\n    \n    # For testing on a directory of images\n    test_on_directory(generator, TEST_DIR, OUTPUT_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}